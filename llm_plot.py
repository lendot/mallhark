import sys
from llama_cpp import Llama

PROMPT_PRE_DEFAULT = "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: "
PROMPT_POST_DEFAULT = " ASSISTANT: "

class LLMPlot:
    """ LLM-based plot generator, using plots generated by tracery as the starting point """

    def __init__(self,
                 model_path,
                 n_ctx = 1024,
                 n_gpu_layers = 0,
                 prompt_pre = PROMPT_PRE_DEFAULT,
                 prompt_post = PROMPT_POST_DEFAULT):
        self.model_path = model_path
        self.n_ctx = n_ctx
        self.n_gpu_layers = n_gpu_layers
        self.prompt_pre = prompt_pre
        self.prompt_post = prompt_post
        self.llama = self._load_model(self.model_path)


    def _load_model(self,model_path):
        print(f'loading {model_path}...')
        llama = Llama(model_path = model_path,n_ctx = self.n_ctx, n_gpu_layers = self.n_gpu_layers, seed = 1225)
        print("Done.")
        return llama

    def _post_process(self,plot):
        plot = plot.replace("Hallmark","Hellmark")
        plot = plot.replace("\n"," ")
        return plot

    def generate(self, prompt, top_k = 40, top_p = 0.9, temperature = 0.9, max_tokens = 300, style = None):

        self.llama.reset()
        style_str = ""
        if style is not None:
            style_str = f' in a {style} style'

        plot_instructions = self.prompt_pre + f'Write a description in 150 words or less for the following Hallmark Christmas Movie{style_str}: ' + prompt + self.prompt_post 
        print(plot_instructions) 

        completion_chunks = self.llama.create_completion(
                plot_instructions,
                top_k = top_k,
                top_p = top_p,
                temperature = temperature,
                max_tokens = max_tokens,
                stream = True
        )

        output = ""

        for completion_chunk in completion_chunks:
            print('',end='',flush=True)
            text = completion_chunk['choices'][0]['text']
            output += text
            print(text,end='',flush=True)

        return self._post_process(output)

