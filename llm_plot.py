import sys
from llama_cpp import Llama

PROMPT_PRE = "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\n\n### Human: " 
PROMPT_POST = "\n### Assistant: "

class LLMPlot:
    """ LLM-based plot generator, using plots generated by tracery as the starting point """

    def __init__(self, model_path, n_ctx = 1024, n_gpu_layers=0):
        self.model_path = model_path
        self.n_ctx = n_ctx
        self.n_gpu_layers = n_gpu_layers
        self.llama = self._load_model(self.model_path)


    def _load_model(self,model_path):
        print(f'loading {model_path}...')
        llama = Llama(model_path = model_path,n_ctx = self.n_ctx, n_gpu_layers = self.n_gpu_layers, seed = 1225)
        print("Done.")
        return llama

    def _post_process(self,plot):
        plot = plot.replace("Hallmark","Hellmark")
        return plot

    def generate(self, prompt, top_k = 40, top_p = 0.9, temperature = 0.9, max_tokens = 300):

        self.llama.reset()

        plot_instructions = PROMPT_PRE + "Write a description in 100 words or less for the following Hallmark Movie: " + prompt + PROMPT_POST 
        print(plot_instructions) 

        completion_chunks = self.llama.create_completion(
                plot_instructions,
                top_k = top_k,
                top_p = top_p,
                temperature = temperature,
                max_tokens = max_tokens,
                stream = True
        )

        output = ""

        for completion_chunk in completion_chunks:
            print('',end='',flush=True)
            text = completion_chunk['choices'][0]['text']
            output += text
            print(text,end='',flush=True)

        return self._post_process(output)

